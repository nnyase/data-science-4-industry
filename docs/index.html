<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">website</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a>
  <ul class="collapse">
  <li><a href="#data-exploration" id="toc-data-exploration" class="nav-link" data-scroll-target="#data-exploration">Data Exploration</a>
  <ul class="collapse">
  <li><a href="#data-imbalances" id="toc-data-imbalances" class="nav-link" data-scroll-target="#data-imbalances">Data Imbalances</a></li>
  </ul></li>
  <li><a href="#data-cleaning" id="toc-data-cleaning" class="nav-link" data-scroll-target="#data-cleaning">Data Cleaning</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a>
  <ul class="collapse">
  <li><a href="#bag-of-wordscount-vectorization" id="toc-bag-of-wordscount-vectorization" class="nav-link" data-scroll-target="#bag-of-wordscount-vectorization">Bag of Words/Count Vectorization</a></li>
  <li><a href="#term-frequency-inverse-document-frequency-tf-idf" id="toc-term-frequency-inverse-document-frequency-tf-idf" class="nav-link" data-scroll-target="#term-frequency-inverse-document-frequency-tf-idf">Term Frequency-Inverse Document Frequency (TF-IDF)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a>
  <ul class="collapse">
  <li><a href="#naive-bayes-classifier" id="toc-naive-bayes-classifier" class="nav-link" data-scroll-target="#naive-bayes-classifier">Naive Bayes Classifier</a></li>
  <li><a href="#xgboost" id="toc-xgboost" class="nav-link" data-scroll-target="#xgboost">XGBoost</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers">Transformers</a>
  <ul class="collapse">
  <li><a href="#bidirectional-encoder-representation-transformer-bert" id="toc-bidirectional-encoder-representation-transformer-bert" class="nav-link" data-scroll-target="#bidirectional-encoder-representation-transformer-bert">Bidirectional Encoder Representation Transformer (BERT)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#naive-bayes-classification" id="toc-naive-bayes-classification" class="nav-link" data-scroll-target="#naive-bayes-classification">Naive Bayes Classification</a></li>
  <li><a href="#naive-bayes-classification-with-tf-idf" id="toc-naive-bayes-classification-with-tf-idf" class="nav-link" data-scroll-target="#naive-bayes-classification-with-tf-idf">Naive Bayes Classification with TF-IDF</a></li>
  <li><a href="#xgboost-1" id="toc-xgboost-1" class="nav-link" data-scroll-target="#xgboost-1">XGBoost</a></li>
  <li><a href="#xgboost-with-tf-idf" id="toc-xgboost-with-tf-idf" class="nav-link" data-scroll-target="#xgboost-with-tf-idf">XGBoost with TF-IDF</a></li>
  <li><a href="#feed-forward-neural-network" id="toc-feed-forward-neural-network" class="nav-link" data-scroll-target="#feed-forward-neural-network">Feed Forward Neural Network</a></li>
  <li><a href="#bert-transformer-language-model" id="toc-bert-transformer-language-model" class="nav-link" data-scroll-target="#bert-transformer-language-model">BERT (Transformer Language Model)</a></li>
  </ul></li>
  <li><a href="#discussion-and-conclusion" id="toc-discussion-and-conclusion" class="nav-link" data-scroll-target="#discussion-and-conclusion">Discussion and Conclusion</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">website</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="abstract" class="level1">
<h1>Abstract</h1>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>A State of Nation Address (SONA) is a speech given by the President of South Africa in which the president reports on the status of the nation. It functions as an annual report that include political and socio-economic topics. These include but not limited to topics surrounding the nation’s budget, economy, news, agenda, progress, achievements and the president’s priorities and legislative proposals. The SONA gives us an idea of broader political landscape and socio-economic issues and challenges that are plaguing the country. At the same time, it also reflect on the achievements and growth and progress the country has made in the past year. Consequently, the nuances in the content, style, and delivery of these addresses provide insightful glimpses into the leadership styles and priorities of different presidents. Thus, developing a machine learning model to figure out which president said a particular statement is avenue to assess the predictive power of these models in a Natural Language Processing task. In addition to this, it may provides insight in linguistic signatures of the various presidents and if they are distinct enough to be identified by a machine learning model.</p>
<p>In this assignment, we will be comparing the performance of different machine learning models in the task of authorship attribution. Author attribution is the task of identifying the author of a given text. More specifically, we will use the SONA speeches given by the presidents of South Africa from 1994-2022 and predict which president said which statement. In the assignment, we will test multiple machine learning models including Multinomial Naive Bayes Classifier, eXtreme gradient boosting (XGBoost), feed forward neural network and a Bidirectional Encoder Representations from Transformers (BERT) model. We opt on this selection of models due to the assignment requirements but also added in a transformer model due to the recent popularity explosion in large languague models (LLM) such as BART and chatGPT. In addition to this, we will also vary aspects of the training data by using various text mining and preprocessing techniques such as count vectorization, tf-idf vectorization, and training attention masks for the BERT model. By doing so, we aim to construct models capable of attributing specific sentences to their respective presidents and comparing the performance of those respective models.</p>
<p>A related study on authorship attribution using machine learning <span class="citation" data-cites="ramnial2016authorship">[@ramnial2016authorship]</span>. The study compared a two machine learning models namely K-Nearest Neighbour, and Sequential minimal optimization to assign PhD thesis to their corresponding authors.</p>
<p>Moreover, there is a survey on different statistical and machine learning techniques for author attribution. The survey compares and contrasts different techniques to each other and also highlights important parameters such as length of corpus, number of candidate authors and distribution of training corpus over the authors (balanced or imbalanced) . All these factors will be considered for the current analyzes <span class="citation" data-cites="stamatatos2009survey">[@stamatatos2009survey]</span>.</p>
<p>Furthermore, with the recent popularity of LLM, transformer approaches recently been explored for authorship attribution. <span class="citation" data-cites="fabien2020bertaa">@fabien2020bertaa</span> fine-tuned a pre-trained BERT for author classification. The BERT model performs the best when sufficient training data per author are available, there is no large class imbalance.</p>
<p>The relevant work showcases that researchers have found significant results when discerning which body of text belongs to which person using a variety of machine learning and deep learning techniques. The following study aims to take inspiration from all the mentioned related works to implement multiple deep learning and machine learning approaches to predict which South African president said which sentence as apart of the SONA speeches.</p>
</section>
<section id="data" class="level1">
<h1>Data</h1>
<p>The primary dataset utilized in this study comprises the textual transcripts of the State of the Nation Address (SONA) delivered by various South African presidents from the year 1994 to 2022. These transcripts were sourced from the official government website (<a href="https://www.gov.za/state-nation-address" class="uri">https://www.gov.za/state-nation-address</a>). The presidents represented in the SONA speeches are F.W deKlerk, Nelson Mandela, Thabo Mbeki, Jacob Zuma, K. Montlante and Cyril Ramaphosa.</p>
<p>For the purpose of the study, the dataset was divided into training, validation, and test sets with proportions of 70%, 10%, and 20% respectively.</p>
<section id="data-exploration" class="level2">
<h2 class="anchored" data-anchor-id="data-exploration">Data Exploration</h2>
<p>Figure <a href="#fig:TOP15" data-reference-type="ref" data-reference="fig:TOP15">1</a> showcases the frequency of the top 15 words used by different presidents in their State of the Nation Addresses (SONA). The data is curated after removing common stop words to highlight words of significance and value in understanding each president’s primary focuses and concerns during their respective terms.</p>
<div id="fig:TOP15" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="15mostcommon_withoutcommon.png" class="img-fluid figure-img" alt="Frequency of the top 15 words used by different presidents in their SONA."></p>
<p></p><figcaption class="figure-caption">Frequency of the top 15 words used by different presidents in their SONA.</figcaption><p></p>
</figure>
</div>
<section id="data-imbalances" class="level3">
<h3 class="anchored" data-anchor-id="data-imbalances">Data Imbalances</h3>
<p>Data imbalance refers to a situation in classification tasks where the classes are not represented equally in the dataset. Data imbalance stands as a significant challenge in the context of our assignment as the presidents do not have equal representation in the datasets. This issue becomes evident when examining the distribution of sentences across different South African presidents. For instance, while De Klerk has a mere 97 sentences from one speech, other presidents like Mandela and Ramaphosa exceed 1000 sentences, and both Mbeki and Zuma surpass the 2000 mark. This stark disparity in data distribution can jeopardize the efficacy of our classifiers.</p>
<div id="tab:president_sentences_flipped">
<table class="table">
<caption>Number of sentences in SONA speeches for each South African president from 1994 to 2022.</caption>
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><strong>Mandela</strong></th>
<th style="text-align: center;"><strong>Mbeki</strong></th>
<th style="text-align: center;"><strong>Motlanthe</strong></th>
<th style="text-align: center;"><strong>Zuma</strong></th>
<th style="text-align: center;"><strong>Ramaphosa</strong></th>
<th style="text-align: center;"><strong>de Klerk</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>No Sentences</strong></td>
<td style="text-align: center;">1668</td>
<td style="text-align: center;">2397</td>
<td style="text-align: center;">264</td>
<td style="text-align: center;">2629</td>
<td style="text-align: center;">2281</td>
<td style="text-align: center;">97</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="data-cleaning" class="level2">
<h2 class="anchored" data-anchor-id="data-cleaning">Data Cleaning</h2>
<p>In the assignment, a vital preprocessing step involves cleaning the textual data. We implement functions to standardize and sanitize the text data by undergoing a series of transformations. Initially, the text is converted to lowercase to maintain uniformity. We then eliminate any text within square brackets and strip off URLs, ensuring that the content is free from external links or irrelevant metadata. HTML tags. Moreover, we’ve also incorporated the removal of commonly used words or ‘stop words’ that do not carry significant meaning in the context of text analytics. Furthermore, the text data undergoes stemming using the Snowball Stemmer, a process that trims words down to their base or root form, thereby ensuring different forms of a word are treated as one.</p>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>In the study we will explore into two widely recognized text representation techniques: Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TFIDF). Preprocessing data using BoW and TFIDF is crucial for machine learning classification because it transforms textual data into numerical vectors, enabling algorithms to effectively identify patterns and relationships. While BoW emphasizes the frequency of words in a document, disregarding the order or structure, TFIDF goes a step further by weighing the terms based on their significance across a collection of documents.</p>
<p>We will compare both of the text representation techniques for the Naive Bayes, and XGBoost classifier in the task of identifying which president said which sentence.</p>
<section id="bag-of-wordscount-vectorization" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-wordscount-vectorization">Bag of Words/Count Vectorization</h3>
<p>Bag of words is a technique used in natural language processing and text mining to convert text data into numerical format.</p>
<p>This process begins with tokenization, where the text is parsed into individual words or tokens. Following this, a vocabulary of distinct tokens is constructed, serving as a reference point for translating texts into vectors. When encoding text as a vector, the length corresponds to the vocabulary’s size, with each element indicating the frequency of a particular word in the text. For instance, given a vocabulary “apple”, “banana”, “cherry”, the text “apple banana apple” would be vectorized as [2,1,0].</p>
</section>
<section id="term-frequency-inverse-document-frequency-tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="term-frequency-inverse-document-frequency-tf-idf">Term Frequency-Inverse Document Frequency (TF-IDF)</h3>
<p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects the importance of a word in a document relative to a collection or corpus of documents. It is widely used in the domain of information retrieval and text mining.</p>
<p>TF-IDF is composed of two components:</p>
<ol type="1">
<li><p><strong>Term Frequency (TF)</strong>: This quantifies how frequently a term appears in a document. It is calculated as: <span class="math display">\[TF(t) = \frac{\text{Number of times term } t \text{ appears in a document}}{\text{Total number of terms in the document}}\]</span></p></li>
<li><p><strong>Inverse Document Frequency (IDF)</strong>: This measures the importance of a term. If a term is frequent across many documents, it may not be a unique identifier and may not be useful in distinguishing between documents. IDF is determined as: <span class="math display">\[IDF(t) = \log\left(\frac{\text{Total number of documents}}{\text{Number of documents containing term } t}\right)\]</span></p></li>
</ol>
<p>The product of TF and IDF gives the TF-IDF weight of a term in a document. A high TF-IDF score means the term is significant in the given document when compared to the corpus.</p>
</section>
</section>
</section>
<section id="models" class="level1">
<h1>Models</h1>
<section id="naive-bayes-classifier" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<p>Given a set of features <span class="math inline">\(\mathbf{x}\)</span> and a target class <span class="math inline">\(C\)</span>, the Bayes theorem calculates the posterior probability <span class="math inline">\(P(C|\mathbf{x})\)</span> as:</p>
<p><span class="math display">\[P(C|\mathbf{x}) = \frac{P(\mathbf{x}|C)P(C)}{P(\mathbf{x})}\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(P(C|\mathbf{x})\)</span> is the posterior probability of class <span class="math inline">\(C\)</span> given the features <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math inline">\(P(\mathbf{x}|C)\)</span> is the likelihood of features <span class="math inline">\(\mathbf{x}\)</span> given class <span class="math inline">\(C\)</span>.</p></li>
<li><p><span class="math inline">\(P(C)\)</span> is the prior probability of class <span class="math inline">\(C\)</span>.</p></li>
<li><p><span class="math inline">\(P(\mathbf{x})\)</span> is the evidence, which acts as a normalization constant.</p></li>
</ul>
<p>In the Naive Bayes classifier, the likelihood term <span class="math inline">\(P(\mathbf{x}|C)\)</span> is computed by assuming feature independence and multiplying the individual likelihoods for each feature.</p>
</section>
<section id="xgboost" class="level2">
<h2 class="anchored" data-anchor-id="xgboost">XGBoost</h2>
<p>XGBoost stands for Extreme Gradient Boosting, and it’s a powerful and efficient implementation of the gradient boosting framework. It’s widely recognized for its performance and speed. XGBoost works by iteratively adding a set of weak learners (usually decision trees) to model the residual errors from previous iterations.</p>
<p>The core principle behind XGBoost is to correct the mistakes of the previous trees by focusing more on misclassified points (by assigning them higher weights). This process is achieved through a mathematical framework that involves the calculation of gradient and hessian (second-order gradient) for a given loss function.</p>
<p>Given a differentiable loss function <span class="math inline">\(l(y, \hat{y})\)</span>, where <span class="math inline">\(y\)</span> is the true label and <span class="math inline">\(\hat{y}\)</span> is the predicted label, the objective function <span class="math inline">\(\mathcal{L}\)</span> to be optimized in each step can be represented as:</p>
<p><span class="math display">\[\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t)}) + \Omega(f_t)\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the number of data points.</p></li>
<li><p><span class="math inline">\(\hat{y}_i^{(t)}\)</span> is the prediction at the <span class="math inline">\(i^{th}\)</span> iteration.</p></li>
<li><p><span class="math inline">\(\Omega\)</span> is the regularization term to prevent overfitting.</p></li>
<li><p><span class="math inline">\(f_t\)</span> is the model at the <span class="math inline">\(t^{th}\)</span> iteration.</p></li>
</ul>
<p>The regularization term, and its importance in controlling the complexity of individual trees, differentiates XGBoost from other gradient boosting algorithms.</p>
<p>In the assignment, we employed the XGBoost model for classification. The model was tuned with a learning rate of 0.1, a maximum depth of 7 for the trees, and an ensemble of 80 estimators. A grid search to search for the optimal hyperparameters can be conducted. However, this is elaborated in the limitations section.</p>
</section>
<section id="neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks">Neural Networks</h2>
<p>Neural networks are a type of deep learning algorithms used for pattern recognition and classification. A neural network is composed of layers of interconnected nodes called neurons. Data enters from the input layer, and as it traverses through the network’s hidden layers, each neuron processes the data, applying a weighted sum and a non-linear activation function. The final layer, known as the output layer, produces the prediction.</p>
<p>Mathematically, for a given neuron, its output <span class="math inline">\(y\)</span> is a function of the weighted sum of its inputs <span class="math inline">\(x\)</span> and its bias <span class="math inline">\(b\)</span>. The formula for a single neuron can be represented as:</p>
<p><span class="math display">\[y = f\left( \sum_{i} w_i x_i + b \right)\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(w_i\)</span> is the weight of the <span class="math inline">\(i^{th}\)</span> input,</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i^{th}\)</span> input,</p></li>
<li><p><span class="math inline">\(b\)</span> is the bias,</p></li>
<li><p>and <span class="math inline">\(f\)</span> is the activation function, which introduces non-linearity, enabling the network to capture complex patterns.</p></li>
</ul>
<p>Training a neural network involves adjusting the weights and biases using optimization techniques, such as gradient descent, to minimize the error between the predicted and actual outputs.</p>
<p>In the case of the assignment a feedforward neural network is used, where connections between nodes do not form cycles or backpropogate, restricting information to flow in one direction, from input to output.</p>
<p>Figure <a href="#fig:ffnn" data-reference-type="ref" data-reference="fig:ffnn">2</a> illustrates the neural network used in the assignment. The neural network used</p>
<div id="fig:ffnn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ffnn.png" class="img-fluid figure-img" alt="Feed Forward Neural Network used in the assignment"></p>
<p></p><figcaption class="figure-caption">Feed Forward Neural Network used in the assignment</figcaption><p></p>
</figure>
</div>
</section>
<section id="transformers" class="level2">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<p>Transformers are a type of deep learning architecture introduced by researchers at Google in a 2017 paper titled “Attention is All You Need.” They have become foundational for a range of Natural Language Processing (NLP) tasks, including translation, summarization, and various other applications.</p>
<p>Unlike traditional recurrent or convolutional layers, Transformers use a self-attention mechanism to weigh input sequences that embed context.</p>
<p>In the assignment, we will utilize a pre-trained Bidirectional Encoder Representations from Transformer (BERT) model and engage in a fine-tuning process to adapt it to our specific task, which is to predict which president uttered a given sentence. BERT is a deep learning model for natural language processing tasks, developed by Google. It reads text bidirectionally to understand the context better, which significantly enhances its performance on various NLP tasks.</p>
<p>Figure <a href="#fig:enter-BERT_ARCH" data-reference-type="ref" data-reference="fig:enter-BERT_ARCH">3</a> demonstrates the model BERT architecture utilized in this assignment. The BERT model was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia</p>
<section id="bidirectional-encoder-representation-transformer-bert" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-encoder-representation-transformer-bert">Bidirectional Encoder Representation Transformer (BERT)</h3>
<div id="fig:enter-BERT_ARCH" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="bert_arch.png" class="img-fluid figure-img" alt="BERT Model used in assignment"></p>
<p></p><figcaption class="figure-caption">BERT Model used in assignment</figcaption><p></p>
</figure>
</div>
</section>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="naive-bayes-classification" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-classification">Naive Bayes Classification</h2>
<div id="fig:nb" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="confmatrix_NB.png" class="img-fluid figure-img" alt="Confusion Matrix for the Naive Bayes Classifier"></p>
<p></p><figcaption class="figure-caption">Confusion Matrix for the Naive Bayes Classifier</figcaption><p></p>
</figure>
</div>
</section>
<section id="naive-bayes-classification-with-tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-classification-with-tf-idf">Naive Bayes Classification with TF-IDF</h2>
<div id="fig:nbwp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="conf_nbwp.png" class="img-fluid figure-img" alt="Confusion Matrix for the Naive Bayes Classifier with TF-IDF"></p>
<p></p><figcaption class="figure-caption">Confusion Matrix for the Naive Bayes Classifier with TF-IDF</figcaption><p></p>
</figure>
</div>
</section>
<section id="xgboost-1" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-1">XGBoost</h2>
<div id="fig:xgboostresults_pp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="conf_xgboostwp.png" class="img-fluid figure-img" alt="Confusion Matrix for the XGBoost Model with TF-IDF"></p>
<p></p><figcaption class="figure-caption">Confusion Matrix for the XGBoost Model with TF-IDF</figcaption><p></p>
</figure>
</div>
</section>
<section id="xgboost-with-tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-with-tf-idf">XGBoost with TF-IDF</h2>
</section>
<section id="feed-forward-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="feed-forward-neural-network">Feed Forward Neural Network</h2>
<p>H <img src="lossandtrainingnn.png" class="img-fluid" alt="image"></p>
</section>
<section id="bert-transformer-language-model" class="level2">
<h2 class="anchored" data-anchor-id="bert-transformer-language-model">BERT (Transformer Language Model)</h2>
<div id="fig:transformer_results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="lossacc.png" class="img-fluid figure-img" alt="Validation and Training Loss and Accuracy for Transformer"></p>
<p></p><figcaption class="figure-caption">Validation and Training Loss and Accuracy for Transformer</figcaption><p></p>
</figure>
</div>
<div id="tab:accuracy">
<table class="table">
<caption>Training and Test Accuracy of Different Models</caption>
<colgroup>
<col style="width: 47%">
<col style="width: 27%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: center;"><strong>Training Accuracy</strong></th>
<th style="text-align: center;"><strong>Test Accuracy</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Naive Bayes Classifier BoW</td>
<td style="text-align: center;">71.77</td>
<td style="text-align: center;">58.14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes Classifier with TF-IDF</td>
<td style="text-align: center;">71.76</td>
<td style="text-align: center;">54.67</td>
</tr>
<tr class="odd">
<td style="text-align: left;">XGBoost (BoW)</td>
<td style="text-align: center;">71.76</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">XGBoost with TF-IDF</td>
<td style="text-align: center;">71.77</td>
<td style="text-align: center;">50.77</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Feed Forward Neural Network (2 Layers)</td>
<td style="text-align: center;">90.64</td>
<td style="text-align: center;">67.57</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feed Forward Neural Network (4 Layers)</td>
<td style="text-align: center;">66.17</td>
<td style="text-align: center;">56.51</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="discussion-and-conclusion" class="level1">
<h1>Discussion and Conclusion</h1>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<p>- Word embeddings, As mentioned in the results, we believe that sentence structure is too finite to predict which sequence was said by which president. Therefore, we suffered from Furthermore, other factors that could have been the reason for poor performace are that a hyperparameter search was not conducted with the NN models, due to computational constraints and time constraints. A factor that could have hampered the performance are the fact that the data is unbalanced. Given the fact that deKlerk and Mothlanthe were severely underrepresented in the data, this might have affected the models’ performative ability. To mitigate that an imbalanced dataset resampling methods such as Synthetic Minority Over-sampling Technique (SMOTE) where you create synthetic samples in the dataset for the underrepresented class. Resampling methods are typically beneficial for addressing class imbalances, however they must be used with caution as they may lead to overfitting and loss of information.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>